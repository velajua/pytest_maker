# Automated Test Generation with YAML and Python
This repository provides a script for generating automated tests using a YAML input file and Python code. The script generates pytest test functions for each test case specified in the YAML file.

### Installation
To use this script, you'll need to install the required dependencies:

```python
pip install -r requirements.txt
```

### Usage
To use the test generator, create a YAML file with test cases and pass the name of the Python module to test as a command line argument when running the `pytest_maker.py` script:

```python
python pytest_maker.py my_module
```

This will generate a `test_my_module.py` file with pytest test functions.

### Example
As an example, let's say we have a module called `my_module.py` with the following functions:

```python
def add(x, y):
    return x + y

def subtract(x, y):
    return x - y

def multiply(x, y):
    return x * y

def divide(x, y):
    return x / y

def concat_str(a, b):
    return a + b

def concat_list(a, b):
    return a + b
```

### YAML Input File
The input.yaml file is used to specify the test cases that will be generated by the test_maker.py script. Each test case is defined as a key-value pair in the format:

```yaml
function_name$test_name:
  argument: $value1$value2...$valueN
  expected: expected_output_value
  outtype: output_type
  skip: message_to_skip
  fail: message_to_fail
```

Where:

`function_name` is the name of the function to be tested
`test_name` is a unique identifier for the test case
`args` is a string that represents the arguments to be passed to the function, separated by a dollar sign ($)
`expected` is the expected output of the function for the given arguments
`outtype` (optional)` is the expected output data type
`skip` (optional) is a message to skip the test
`fail` (optional) is a message to indicate the test has failed
Note: only one of skip or fail can be used per test case.

Here's an example input.yaml file:

```yaml
add$simple_add:
  skip: 'The function should be skipped'
  args: $2$3
  expected: 5
  outtype: int
subtract$simple_subtract:
  args: $3$3
  expected: 0
  outtype: int
multiply$1:
  fail: 'The value is not true'
  args: $1$7
  expected: 8
  outtype: int
```

Then, running `python pytest_maker.py my_module` will generate a file test_my_module.py with the following content which can then be run using the `pytest` library:

```python
import pytest

from typing import *
from my_module import *


@pytest.mark.skip(
    reason="The function should be skipped")
def test_add_simple_add():
    result = add(2, 3)
    assert isinstance(result, int)
    assert result == 5


def test_subtract_simple_subtract():
    result = subtract(3, 3)
    assert isinstance(result, int)
    assert result == 0


@pytest.mark.xfail(
    reason="The value is not true")
def test_multiply_1():
    result = multiply(1, 7)
    assert isinstance(result, int)
    assert result == 8
```
